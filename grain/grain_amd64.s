// Code generated by command: go run asm.go -out out/grain_amd64.s -stubs out/stub_amd64.go -pkg grain. DO NOT EDIT.

// +build gc,!purego

#include "textflag.h"

// func next(s *state) uint32
TEXT ·next(SB), NOSPLIT, $0-12
	// Load state
	MOVQ s+0(FP), AX

	// LFSR shifts
	// Load lfsr: (lo, hi)
	MOVQ 16(AX), CX
	MOVQ 24(AX), DX

	// Load lfsr words (u0, u1, u2, u3)
	// u0 = r.lo
	MOVQ CX, BX

	// u1 = r.lo>>32 | r.hi<<32
	MOVQ CX, SI
	SHRQ $0x20, SI
	MOVQ DX, DI
	SHLQ $0x20, DI
	ORQ  DI, SI

	// u2 = r.hi
	MOVQ DX, DI

	// u3 = r.hi>>32
	MOVQ DX, R8
	SHRQ $0x20, R8

	// v := ln ^ ln3
	MOVQ BX, R9
	XORQ R8, R9

	// <temp> = ln1 ^ ln2
	MOVQ SI, R8
	XORQ DI, R8

	// v ^= (<temp> >> 6)
	SHRQ $0x06, R8
	XORQ R8, R9

	// v ^= (ln0 >> 7)
	MOVQ BX, R8
	SHRQ $0x07, R8
	XORQ R8, R9

	// v ^= (ln2 >> 17)
	MOVQ DI, R8
	SHRQ $0x11, R8
	XORQ R8, R9

	// lo = r.lo>>32 | r.hi<<(64-32)
	SHRQ $0x20, CX
	MOVQ DX, R8
	SHLQ $0x20, R8
	ORQ  R8, CX

	// r.hi>>32 | uint64(x)<<32
	SHRQ $0x20, DX
	MOVQ R9, R8
	SHLQ $0x20, R8
	ORQ  R8, DX

	// Store lfsr: (lo, hi)
	MOVQ CX, 16(AX)
	MOVQ DX, 24(AX)

	// NFSR shifts
	// Load nfsr: (lo, hi)
	MOVQ 32(AX), CX
	MOVQ 40(AX), DX

	// Load nfsr words (u0, u1, u2, u3)
	// u0 = r.lo
	MOVQ CX, R10

	// u1 = r.lo>>32 | r.hi<<32
	MOVQ CX, R11
	SHRQ $0x20, R11
	MOVQ DX, R8
	SHLQ $0x20, R8
	ORQ  R8, R11

	// u2 = r.hi
	MOVQ DX, R12

	// u3 = r.hi>>32
	MOVQ DX, R8
	SHRQ $0x20, R8

	// u := ln0
	MOVQ BX, R9

	// u ^= nn0
	XORQ R10, R9

	// u ^= (nn0 >> 26)
	MOVQ R10, R13
	SHRQ $0x1a, R13
	XORQ R13, R9
	XORQ R8, R9

	// u ^= (nn1 >> 24)
	MOVQ R11, R8
	SHRQ $0x18, R8
	XORQ R8, R9

	// <temp> = (nn0 & nn2) ^ nn2
	MOVQ R11, R8
	ANDQ R10, R8
	XORQ R12, R8

	// u ^= (<temp> >> 27)
	SHRQ $0x1b, R8
	XORQ R8, R9

	// <temp> = nn0 & nn2
	MOVQ R12, R8
	ANDQ R10, R8

	// u ^= (<temp> >> 3)
	SHRQ $0x03, R8
	XORQ R8, R9

	// u ^= (nn0 >> 11) & (nn0 >> 13)
	MOVQ R10, R8
	SHRQ $0x0b, R8
	MOVQ R10, R13
	SHRQ $0x0d, R13
	ANDQ R13, R8
	XORQ R8, R9

	// u ^= (nn0 >> 17) & (nn0 >> 18)
	MOVQ R10, R8
	SHRQ $0x11, R8
	MOVQ R10, R13
	SHRQ $0x12, R13
	ANDQ R13, R8
	XORQ R8, R9

	// u ^= (nn1 >> 8) & (nn1 >> 16)
	MOVQ R11, R8
	SHRQ $0x08, R8
	MOVQ R11, R13
	SHRQ $0x10, R13
	ANDQ R13, R8
	XORQ R8, R9

	// u ^= (nn1 >> 29) & (nn2 >> 1)
	MOVQ R11, R8
	SHRQ $0x1d, R8
	MOVQ R12, R13
	SHRQ $0x01, R13
	ANDQ R13, R8
	XORQ R8, R9

	// u ^= (nn2 >> 4) & (nn2 >> 20)
	MOVQ R12, R8
	SHRQ $0x04, R8
	MOVQ R12, R13
	SHRQ $0x14, R13
	ANDQ R13, R8
	XORQ R8, R9

	// u ^= (nn2 >> 24) & (nn2 >> 28) & (nn2 >> 29) & (nn2 >> 31)
	MOVQ R12, R8
	SHRQ $0x18, R8
	MOVQ R12, R13
	SHRQ $0x1c, R13
	ANDQ R13, R8
	MOVQ R12, R13
	SHRQ $0x1d, R13
	ANDQ R13, R8
	MOVQ R12, R13
	SHRQ $0x1f, R13
	ANDQ R13, R8
	XORQ R8, R9

	// u ^= (nn0 >> 22) & (nn0 >> 24) & (nn0 >> 25)
	MOVQ R10, R8
	SHRQ $0x16, R8
	MOVQ R10, R13
	SHRQ $0x18, R13
	ANDQ R13, R8
	MOVQ R10, R13
	SHRQ $0x19, R13
	ANDQ R13, R8
	XORQ R8, R9

	// u ^= (nn2 >> 6) & (nn2 >> 14) & (nn2 >> 18)
	MOVQ R12, R8
	SHRQ $0x06, R8
	MOVQ R12, R13
	SHRQ $0x0e, R13
	ANDQ R13, R8
	MOVQ R12, R13
	SHRQ $0x12, R13
	ANDQ R13, R8
	XORQ R8, R9

	// lo = r.lo>>32 | r.hi<<(64-32)
	SHRQ $0x20, CX
	MOVQ DX, R8
	SHLQ $0x20, R8
	ORQ  R8, CX

	// r.hi>>32 | uint64(x)<<32
	SHRQ $0x20, DX
	MOVQ R9, R8
	SHLQ $0x20, R8
	ORQ  R8, DX

	// Store nfsr: (lo, hi)
	MOVQ CX, 32(AX)
	MOVQ DX, 40(AX)
	MOVQ R10, R9
	SHRQ $0x02, R9

	// x ^= (nn0 >> 15)
	MOVQ R10, AX
	SHRQ $0x0f, AX
	XORQ AX, R9

	// x ^= (nn1 >> 4)
	MOVQ R11, AX
	SHRQ $0x04, AX
	XORQ AX, R9

	// x ^= (nn1 >> 13)
	MOVQ R11, AX
	SHRQ $0x0d, AX
	XORQ AX, R9
	XORQ R12, R9

	// x ^= (nn2 >> 9)
	MOVQ R12, AX
	SHRQ $0x09, AX
	XORQ AX, R9

	// x ^= (nn2 >> 25)
	MOVQ R12, AX
	SHRQ $0x19, AX
	XORQ AX, R9

	// x ^= (ln2 >> 29)
	MOVQ DI, AX
	SHRQ $0x1d, AX
	XORQ AX, R9

	// x ^= (nn0 >> 12) & (ln0 >> 8)
	MOVQ R10, AX
	SHRQ $0x0c, AX
	MOVQ BX, CX
	SHRQ $0x08, CX
	ANDQ CX, AX
	XORQ AX, R9

	// x ^= (ln0 >> 13) & (ln0 >> 20)
	MOVQ BX, AX
	SHRQ $0x0d, AX
	MOVQ BX, CX
	SHRQ $0x14, CX
	ANDQ CX, AX
	XORQ AX, R9

	// x ^= (nn2 >> 31) & (ln1 >> 10)
	MOVQ R12, AX
	SHRQ $0x1f, AX
	MOVQ SI, CX
	SHRQ $0x0a, CX
	ANDQ CX, AX
	XORQ AX, R9

	// x ^= (ln1 >> 28) & (ln2 >> 15)
	MOVQ SI, AX
	SHRQ $0x1c, AX
	MOVQ DI, CX
	SHRQ $0x0f, CX
	ANDQ CX, AX
	XORQ AX, R9

	// x ^= (nn0 >> 12) & (nn2 >> 31) & (ln2 >> 30)
	MOVQ R10, AX
	SHRQ $0x0c, AX
	MOVQ R12, CX
	SHRQ $0x1f, CX
	ANDQ CX, AX
	MOVQ DI, CX
	SHRQ $0x1e, CX
	ANDQ CX, AX
	XORQ AX, R9

	// Store result
	MOVL R9, ret+8(FP)
	RET

// func accumulate(reg uint64, acc uint64, ms uint16, pt uint16) (reg1 uint64, acc1 uint64)
TEXT ·accumulate(SB), NOSPLIT, $0-40
	MOVQ    reg+0(FP), AX
	MOVQ    acc+8(FP), CX
	MOVWLZX pt+18(FP), DX
	MOVWQZX ms+16(FP), BX

	// var acctmp uint16
	XORQ SI, SI

	// regtmp := uint32(ms) << 16
	MOVWLZX BX, DI
	SHLL    $0x10, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// regtmp >>= 1
	SHRL $0x01, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// regtmp >>= 1
	SHRL $0x01, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// regtmp >>= 1
	SHRL $0x01, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// regtmp >>= 1
	SHRL $0x01, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// regtmp >>= 1
	SHRL $0x01, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// regtmp >>= 1
	SHRL $0x01, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// regtmp >>= 1
	SHRL $0x01, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// regtmp >>= 1
	SHRL $0x01, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// regtmp >>= 1
	SHRL $0x01, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// regtmp >>= 1
	SHRL $0x01, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// regtmp >>= 1
	SHRL $0x01, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// regtmp >>= 1
	SHRL $0x01, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// regtmp >>= 1
	SHRL $0x01, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// regtmp >>= 1
	SHRL $0x01, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// regtmp >>= 1
	SHRL $0x01, DI

	// mask := -uint64(pt & 1)
	MOVL DX, R8
	ANDL $0x01, R8
	NEGQ R8

	// acc ^= reg & mask
	MOVQ AX, R9
	ANDQ R8, R9
	XORQ R9, CX

	// acctmp ^= uint16(regtmp) & uint16(mask)
	ANDW DI, R8
	XORW R8, SI

	// reg >>= 1
	SHRQ $0x01, AX

	// pt >>= 1
	SHRL $0x01, DX

	// reg |= uint64(ms) << 48
	// acc ^= uint64(acctmp) << 48
	SHLQ $0x30, BX
	SHLQ $0x30, SI
	ORQ  BX, AX
	XORQ SI, CX

	// Store results
	MOVQ AX, reg1+24(FP)
	MOVQ CX, acc1+32(FP)
	RET
